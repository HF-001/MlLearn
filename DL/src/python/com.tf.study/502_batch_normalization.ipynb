{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Know more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "\n",
    "Dependencies:\n",
    "tensorflow: 1.1.0\n",
    "matplotlib\n",
    "numpy\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Hyper parameters\n",
    "N_SAMPLES = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 12\n",
    "LR = 0.03\n",
    "N_HIDDEN = 8\n",
    "ACTIVATION = tf.nn.tanh\n",
    "B_INIT = tf.constant_initializer(-0.2)      # use a bad bias initialization\n",
    "\n",
    "# training data\n",
    "x = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\n",
    "np.random.shuffle(x)\n",
    "noise = np.random.normal(0, 2, x.shape)\n",
    "y = np.square(x) - 5 + noise\n",
    "train_data = np.hstack((x, y))\n",
    "\n",
    "# test data\n",
    "test_x = np.linspace(-7, 10, 200)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 2, test_x.shape)\n",
    "test_y = np.square(test_x) - 5 + noise\n",
    "\n",
    "# plot input data\n",
    "plt.scatter(x, y, c='#FF9359', s=50, alpha=0.5, label='train')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# tensorflow placeholder\n",
    "tf_x = tf.placeholder(tf.float32, [None, 1])\n",
    "tf_y = tf.placeholder(tf.float32, [None, 1])\n",
    "tf_is_train = tf.placeholder(tf.bool, None)     # flag for using BN on training or testing\n",
    "\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, batch_normalization=False):\n",
    "        self.is_bn = batch_normalization\n",
    "\n",
    "        self.w_init = tf.random_normal_initializer(0., .1)  # weights initialization\n",
    "        self.pre_activation = [tf_x]\n",
    "        if self.is_bn:\n",
    "            self.layer_input = [tf.layers.batch_normalization(tf_x, training=tf_is_train)]  # for input data\n",
    "        else:\n",
    "            self.layer_input = [tf_x]\n",
    "        for i in range(N_HIDDEN):  # adding hidden layers\n",
    "            self.layer_input.append(self.add_layer(self.layer_input[-1], 10, ac=ACTIVATION))\n",
    "        self.out = tf.layers.dense(self.layer_input[-1], 1, kernel_initializer=self.w_init, bias_initializer=B_INIT)\n",
    "        self.loss = tf.losses.mean_squared_error(tf_y, self.out)\n",
    "\n",
    "        # !! IMPORTANT !! the moving_mean and moving_variance need to be updated,\n",
    "        # pass the update_ops with control_dependencies to the train_op\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.train = tf.train.AdamOptimizer(LR).minimize(self.loss)\n",
    "\n",
    "    def add_layer(self, x, out_size, ac=None):\n",
    "        x = tf.layers.dense(x, out_size, kernel_initializer=self.w_init, bias_initializer=B_INIT)\n",
    "        self.pre_activation.append(x)\n",
    "        # the momentum plays important rule. the default 0.99 is too high in this case!\n",
    "        if self.is_bn: x = tf.layers.batch_normalization(x, momentum=0.4, training=tf_is_train)    # when have BN\n",
    "        out = x if ac is None else ac(x)\n",
    "        return out\n",
    "\n",
    "nets = [NN(batch_normalization=False), NN(batch_normalization=True)]    # two nets, with and without BN\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# plot layer input distribution\n",
    "f, axs = plt.subplots(4, N_HIDDEN+1, figsize=(10, 5))\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "def plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n",
    "    for i, (ax_pa, ax_pa_bn, ax,  ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n",
    "        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n",
    "        if i == 0: p_range = (-7, 10); the_range = (-7, 10)\n",
    "        else: p_range = (-4, 4); the_range = (-1, 1)\n",
    "        ax_pa.set_title('L' + str(i))\n",
    "        ax_pa.hist(pre_ac[i].ravel(), bins=10, range=p_range, color='#FF9359', alpha=0.5)\n",
    "        ax_pa_bn.hist(pre_ac_bn[i].ravel(), bins=10, range=p_range, color='#74BCFF', alpha=0.5)\n",
    "        ax.hist(l_in[i].ravel(), bins=10, range=the_range, color='#FF9359')\n",
    "        ax_bn.hist(l_in_bn[i].ravel(), bins=10, range=the_range, color='#74BCFF')\n",
    "        for a in [ax_pa, ax, ax_pa_bn, ax_bn]:\n",
    "            a.set_yticks(()); a.set_xticks(())\n",
    "        ax_pa_bn.set_xticks(p_range); ax_bn.set_xticks(the_range); axs[2, 0].set_ylabel('Act'); axs[3, 0].set_ylabel('BN Act')\n",
    "    plt.pause(0.01)\n",
    "\n",
    "losses = [[], []]   # record test loss\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch)\n",
    "    np.random.shuffle(train_data)\n",
    "    step = 0\n",
    "    in_epoch = True\n",
    "    while in_epoch:\n",
    "        b_s, b_f = (step*BATCH_SIZE) % len(train_data), ((step+1)*BATCH_SIZE) % len(train_data) # batch index\n",
    "        step += 1\n",
    "        if b_f < b_s:\n",
    "            b_f = len(train_data)\n",
    "            in_epoch = False\n",
    "        b_x, b_y = train_data[b_s: b_f, 0:1], train_data[b_s: b_f, 1:2]         # batch training data\n",
    "        sess.run([nets[0].train, nets[1].train], {tf_x: b_x, tf_y: b_y, tf_is_train: True})     # train\n",
    "\n",
    "        if step == 1:\n",
    "            l0, l1, l_in, l_in_bn, pa, pa_bn = sess.run(\n",
    "                [nets[0].loss, nets[1].loss, nets[0].layer_input, nets[1].layer_input,\n",
    "                 nets[0].pre_activation, nets[1].pre_activation],\n",
    "                {tf_x: test_x, tf_y: test_y, tf_is_train: False})\n",
    "            [loss.append(l) for loss, l in zip(losses, [l0, l1])]   # recode test loss\n",
    "            plot_histogram(l_in, l_in_bn, pa, pa_bn)     # plot histogram\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "# plot test loss\n",
    "plt.figure(2)\n",
    "plt.plot(losses[0], c='#FF9359', lw=3, label='Original')\n",
    "plt.plot(losses[1], c='#74BCFF', lw=3, label='Batch Normalization')\n",
    "plt.ylabel('test loss'); plt.ylim((0, 2000)); plt.legend(loc='best')\n",
    "\n",
    "# plot prediction line\n",
    "pred, pred_bn = sess.run([nets[0].out, nets[1].out], {tf_x: test_x, tf_is_train: False})\n",
    "plt.figure(3)\n",
    "plt.plot(test_x, pred, c='#FF9359', lw=4, label='Original')\n",
    "plt.plot(test_x, pred_bn, c='#74BCFF', lw=4, label='Batch Normalization')\n",
    "plt.scatter(x[:200], y[:200], c='r', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='best'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
