{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "代码参考自：机器之心\n",
    "主要添加注释\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf  #导入tf\n",
    "import numpy as np\n",
    "hello = tf.constant('Hello, TensorFlow!')  # 定义tf常量\n",
    "sess = tf.Session()       # 创建tf会话\n",
    "print(sess.run(hello))    # 执行会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = tf.constant(2, tf.int16)   # 创建常量2 \n",
    "b = tf.constant(4, tf.float32)\n",
    "c = tf.constant(8, tf.float32)\n",
    "\n",
    "d = tf.Variable(2, tf.int16)   # 创建变量2\n",
    "e = tf.Variable(4, tf.float32)\n",
    "f = tf.Variable(8, tf.float32)\n",
    "\n",
    "g = tf.constant(np.zeros(shape=(2,2), dtype=np.float32)) #可以正常声明变量,[[0,0],[0,0]]\n",
    "\n",
    "h = tf.zeros([11], tf.int16)  # [0,0,0....]\n",
    "i = tf.ones([2,2], tf.float32) # [[1,1],[1,1]]\n",
    "j = tf.zeros([1000,4,3], tf.float64) # 三维0矩阵\n",
    "\n",
    "k = tf.Variable(tf.zeros([2,2], tf.float32)) # 变量矩阵\n",
    "l = tf.Variable(tf.zeros([5,6,5], tf.float32)) # 变量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=() dtype=int32_ref>\n",
      "8\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2, tf.int16)     # 常量2\n",
    "b = tf.constant(4, tf.float32)   # 常量4\n",
    "\n",
    "graph = tf.Graph()   # 创建一个图，可以通过tensorboard用图形化界面展示出来流程结构, 可以整合一段代码为一个整体存在于一个图中             \n",
    "with graph.as_default(): # 返回一个上下文管理器，这个上下文管理器使用这个图作为默认的图\n",
    "    a = tf.Variable(8, tf.float32) # 创建变量8\n",
    "    b = tf.Variable(tf.zeros([2,2], tf.float32)) # 创建[[0,0],[0,0]]\n",
    "    \n",
    "with tf.Session(graph=graph) as session: # 开启会话\n",
    "    tf.global_variables_initializer().run() # 初始化参数\n",
    "    print(f)   # 打印f\n",
    "    print(session.run(a)) \n",
    "    print(session.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,2],name=\"a\")\n",
    "b=tf.constant([2,4],name=\"b\")\n",
    "result = a+b\n",
    "print(result) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6 8]\n",
      "[2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant([1,2,3,4])\n",
    "b=tf.constant([1,2,3,4])\n",
    "result=a+b\n",
    "sess=tf.Session()  # 创建会话一\n",
    "print(sess.run(result))\n",
    "sess.close\n",
    "\n",
    "#输出 [2 4 6 8]\n",
    "\n",
    "with tf.Session() as sess: # 创建会话二\n",
    "    a=tf.constant([1,2,3,4])\n",
    "    b=tf.constant([1,2,3,4])\n",
    "    result=a+b\n",
    "    print(sess.run(result))\n",
    "    \n",
    "#输出 [2 4 6 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11131823  2.3845987 ]]\n",
      "[[-0.11131823  2.3845987 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BaseSession.close of <tensorflow.python.client.session.Session object at 0x7ff18d9d19e8>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1=tf.Variable(tf.random_normal([1,2],stddev=1,seed=1))  # 创建变量，正态分布\n",
    "\n",
    "#因为需要重复输入x，而每建一个x就会生成一个结点，计算图的效率会低。所以使用占位符\n",
    "x=tf.placeholder(tf.float32,shape=(1,2)) # 占位符\n",
    "x1=tf.constant([[0.7,0.9]])       # 常量矩阵\n",
    "\n",
    "a=x+w1                \n",
    "b=x1+w1\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#运行y时将占位符填上，feed_dict为字典，变量名不可变\n",
    "y_1=sess.run(a,feed_dict={x:[[0.7,0.9]]})  # feed_dict字典，传入需要的数据\n",
    "y_2=sess.run(b)  \n",
    "print(y_1)\n",
    "print(y_2)\n",
    "sess.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the distance between [[1 2]] and [[15 16]] -> [19.79899]\n",
      "the distance between [[3 4]] and [[13 14]] -> [14.142136]\n",
      "the distance between [[5 6]] and [[11 12]] -> [8.485281]\n",
      "the distance between [[7 8]] and [[ 9 10]] -> [2.828427]\n"
     ]
    }
   ],
   "source": [
    "list_of_points1_ = [[1,2], [3,4], [5,6], [7,8]]\n",
    "list_of_points2_ = [[15,16], [13,14], [11,12], [9,10]]\n",
    "list_of_points1 = np.array([np.array(elem).reshape(1,2) for elem in list_of_points1_])\n",
    "list_of_points2 = np.array([np.array(elem).reshape(1,2) for elem in list_of_points2_])\n",
    "\n",
    "graph = tf.Graph() # 创建图\n",
    "with graph.as_default():\n",
    "    \n",
    "    #我们使用 tf.placeholder() 创建占位符 ，在 session.run() 过程中再投递数据 \n",
    "    point1 = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "    point2 = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "    \n",
    "    def calculate_eucledian_distance(point1, point2):\n",
    "        difference = tf.subtract(point1, point2) # 减法\n",
    "        power2 = tf.pow(difference, tf.constant(2.0, shape=(1,2))) # 指数\n",
    "        add = tf.reduce_sum(power2) # 求和，把每个位置的值加起来\n",
    "        eucledian_distance = tf.sqrt(add)  # 求根号[(x1-y1）^2+(x2-y2)^2]^(1/2)\n",
    "        return eucledian_distance\n",
    "    \n",
    "    dist = calculate_eucledian_distance(point1, point2)\n",
    "    \n",
    "with tf.Session(graph=graph) as session: # 创建会话\n",
    "    tf.global_variables_initializer().run() # 初始化\n",
    "    for ii in range(len(list_of_points1)):  # 遍历列表，按行\n",
    "        point1_ = list_of_points1[ii] #行1\n",
    "        point2_ = list_of_points2[ii] #行2\n",
    "        \n",
    "        #使用feed_dict将数据投入到[dist]中\n",
    "        feed_dict = {point1 : point1_, point2 : point2_} # 组成字典\n",
    "        distance = session.run([dist], feed_dict=feed_dict) # 计算欧式距离\n",
    "        print(\"the distance between {} and {} -> {}\".format(point1_, point2_, distance)) # 打印结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "在迭代 0 次后，训练损失为 0.308504\n",
      "在迭代 1000 次后，训练损失为 0.0393406\n",
      "在迭代 2000 次后，训练损失为 0.0182158\n",
      "在迭代 3000 次后，训练损失为 0.0104779\n",
      "在迭代 4000 次后，训练损失为 0.00680374\n",
      "在迭代 5000 次后，训练损失为 0.00446512\n",
      "在迭代 6000 次后，训练损失为 0.00296797\n",
      "在迭代 7000 次后，训练损失为 0.00218553\n",
      "在迭代 8000 次后，训练损失为 0.00179452\n",
      "在迭代 9000 次后，训练损失为 0.0013211\n",
      "在迭代 10000 次后，训练损失为 0.000957699\n",
      "在迭代 11000 次后，训练损失为 0.00081103\n",
      "在迭代 12000 次后，训练损失为 0.000643147\n",
      "在迭代 13000 次后，训练损失为 0.00047439\n",
      "在迭代 14000 次后，训练损失为 0.00030086\n",
      "在迭代 15000 次后，训练损失为 0.000137936\n",
      "[[-0.8113182  3.8425548  3.3816528]\n",
      " [-2.4427042  1.9863598  3.5072231]]\n",
      "[[-0.8113182]\n",
      " [ 4.029077 ]\n",
      " [ 2.602852 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size=10\n",
    "w1=tf.Variable(tf.random_normal([2,3],stddev=1,seed=1)) # 创建随机数矩阵2*3\n",
    "w2=tf.Variable(tf.random_normal([3,1],stddev=1,seed=1)) # 创建随机数矩阵3*1\n",
    "\n",
    "\n",
    "# None 可以根据batch 大小确定维度，在shape的一个维度上使用None，方便不大的batch\n",
    "x=tf.placeholder(tf.float32,shape=(None,2))  # 创建占位符\n",
    "y=tf.placeholder(tf.float32,shape=(None,1))  # 创建占位符\n",
    "\n",
    "a=tf.nn.relu(tf.matmul(x,w1))     # x*w1\n",
    "yhat=tf.nn.relu(tf.matmul(a,w2))  # a*w2\n",
    "\n",
    "#定义交叉熵为损失函数，训练过程使用Adam算法最小化交叉熵\n",
    "cross_entropy=-tf.reduce_mean(y*tf.log(tf.clip_by_value(yhat,1e-10,1.0)))  \n",
    "#tf.reduce_mean计算所有元素的均值，如指定axis则沿指定轴取均值。tf.clip_by_value基于定义的min与max对tesor数据进行截断操作，目的是为了应对梯度爆发或者梯度消失的情况\n",
    "\n",
    "train_step=tf.train.AdamOptimizer(0.001).minimize(cross_entropy)   # Adam优化算法：是一个寻找全局最优点的优化算法，引入了二次方梯度校正。\n",
    "\n",
    "rdm=RandomState(1)   # 随机数生成器\n",
    "data_size=512\n",
    "\n",
    "#生成两个特征，共data_size个样本\n",
    "X=rdm.rand(data_size,2)  # 生成随机样本\n",
    "#定义规则给出样本标签，所有x1+x2<1的样本认为是正样本，其他为负样本。Y，1为正样本\n",
    "Y = [[int(x1+x2 < 1)] for (x1, x2) in X]\n",
    "\n",
    "with tf.Session() as sess: # 创建会话\n",
    "    sess.run(tf.global_variables_initializer())  # 初始化\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    steps=15001\n",
    "    for i in range(steps): # 循环\n",
    "        \n",
    "        #选定每一个批量读取的首尾位置，确保在1个epoch内采样训练\n",
    "        start = i * batch_size % data_size\n",
    "        end = min(start + batch_size,data_size)\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y:Y[start:end]}) #启动优化器训练，传入数据\n",
    "        if i % 1000 == 0:\n",
    "            training_loss= sess.run(cross_entropy,feed_dict={x:X,y:Y}) #每1000轮，使用交叉熵函数进行计算输出，评估训练效果\n",
    "            print(\"在迭代 %d 次后，训练损失为 %g\"%(i,training_loss))\n",
    "        if i == steps-1: # 最后一轮\n",
    "            print(sess.run(w1))\n",
    "            print(sess.run(w2))#输出更新后的权重矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码定义了一个简单的三层全连接网络（输入层、隐藏层和输出层分别为2、3和2个神经元），隐藏层和输出层的激活函数使用的是ReLU函数。该模型训练的样本总数为512，每次迭代读取的批量为10。这个简单的全连接网络以交叉熵为损失函数，并使用Adam优化算法进行权重更新。\n",
    "\n",
    "其中需要注意的几个函数如tf.nn.relu()代表调用ReLU激活函数，tf.matmul()为矩阵乘法等。tf.clip_by_value(yhat,1e-10,1.0)这一语句代表的是截断yhat的值，因为这一语句是嵌套在tf.log()函数内的，所以我们需要确保yhat的取值不会导致对数无穷大。\n",
    "\n",
    "tf.train.AdamOptimizer(learning_rate).minimize(cost_function)是进行训练的函数，其中我们采用的是Adam优化算法更新权重，并且需要提供学习速率和损失函数这两个参数。后面就是生成训练数据，X=rdm.rand(512,2)表示随机生成512个样本，每个样本有两个特征值。最后就是迭代运行了，这里我们计算出每一次迭代抽取数据的起始位置（start）和结束位置（end），并且每一次抽取的数据量为前面我们定义的批量，如果一个epoch最后剩余的数据少于批量大小，那就只是用剩余的数据进行训练。最后两句代码是为了计算训练损失并迭代一些次数后输出训练损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle # 用于python特有的类型和python的数据类型间进行转换\n",
    "import json\n",
    "import os\n",
    "\n",
    "#定义一些预处理函数\n",
    "\n",
    "def flatten_tf_array(array):\n",
    "    shape = array.get_shape().as_list() # get_shape()用于获取tensor的大小，返回一个元组，通过as_list()方法转成list\n",
    "    return tf.reshape(array, [shape[0], shape[1] * shape[2] * shape[3]]) # 变换形状\n",
    "\n",
    "def accuracy(predictions, labels): # 计算准确率\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]) # 比较predictions和label，求准确率\n",
    "# np.argmax(a)取出a中元素最大值所对应的索引，np.argmax(a，1)对a按行取最大值索引\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0]) \n",
    "    #函数shuffle与permutation都是对原来的数组进行重新洗牌（即随机打乱原来的元素顺序）；区别在于shuffle直接在原来的数组上进行操作，\n",
    "    #改变原来数组的顺序，无返回值。而permutation不直接在原来的数组上进行操作，而是返回一个新的打乱顺序的数组，并不改变原来的数组。\n",
    "    shuffled_dataset = dataset[permutation, :, :] # 取乱序后的训练集\n",
    "    shuffled_labels = labels[permutation]  # 取乱序后的label\n",
    "    return shuffled_dataset, shuffled_labels  # 返回乱序后的训练集，label\n",
    "\n",
    "def one_hot_encode(np_array):  # one-hot编码, 两个array做==操作，相当于两array逐个元素比较是否相等，相等为1，否则为0.最后形成一个n*10的矩阵，n为np_array的大小\n",
    "    return (np.arange(10) == np_array[:,None]).astype(np.float32) # np.arange(10)返回1-9的数组，astype类型转换\n",
    "\n",
    "def reformat_data(dataset, labels, image_width, image_height, image_depth):\n",
    "    # 遍历图片集，转成w*h*d, 再整成数组\n",
    "    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) for image_data in dataset])\n",
    "    # 对label进行one-hot编码\n",
    "    np_labels_ = one_hot_encode(np.array(labels, dtype=np.float32))\n",
    "    np_dataset, np_labels = randomize(np_dataset_, np_labels_) # 乱序\n",
    "    return np_dataset, np_labels # 返回训练集，label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集包含以下标签: [0 1 2 3 4 5 6 7 8 9]\n",
      "训练集维度 (50000, 32, 32, 3) (50000, 10)\n",
      "测试集维度 (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "cifar10_folder = './cifar-10-batches-py/' # 数据路径\n",
    "train_datasets = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', ] # 训练集\n",
    "test_dataset = ['test_batch'] # 测试集\n",
    "c10_image_height = 32 # 图片高度\n",
    "c10_image_width = 32  # 图片宽度\n",
    "c10_image_depth = 3   # 图片深度\n",
    "c10_num_labels = 10   # label种类数\n",
    "c10_image_size = 32 #Ahmet Taspinar的代码缺少了这一语句。图片大小？\n",
    "\n",
    "with open(cifar10_folder + test_dataset[0], 'rb') as f0: # 载入数据,以二进制形式打开文件\n",
    "    c10_test_dict = pickle.load(f0, encoding='bytes')    # 反序列化对象，将文件中的数据解析为一个python对象\n",
    "\n",
    "c10_test_dataset, c10_test_labels = c10_test_dict[b'data'], c10_test_dict[b'labels'] # 分割测试集的feature，label\n",
    "#改变图片形状、对label进行one-hot编码，乱序\n",
    "test_dataset_cifar10, test_labels_cifar10 = reformat_data(c10_test_dataset, c10_test_labels, c10_image_size, c10_image_size, c10_image_depth)\n",
    "\n",
    "c10_train_dataset, c10_train_labels = [], [] # 存训练集的容器\n",
    "for train_dataset in train_datasets:  # 遍历训练集，训练集分成了5份，所以需要遍历，合并到一起\n",
    "    with open(cifar10_folder + train_dataset, 'rb') as f0: # 以二进制形式打开文件\n",
    "        c10_train_dict = pickle.load(f0, encoding='bytes') # 反序列化对象\n",
    "        c10_train_dataset_, c10_train_labels_ = c10_train_dict[b'data'], c10_train_dict[b'labels'] # 分割测试集的feature，label\n",
    " \n",
    "        c10_train_dataset.append(c10_train_dataset_) #合并特征\n",
    "        c10_train_labels += c10_train_labels_  #合并label\n",
    "\n",
    "c10_train_dataset = np.concatenate(c10_train_dataset, axis=0) # 数组拼接，沿纵轴，将多个list进行拼接\n",
    "#改变图片形状、对label进行one-hot编码，乱序\n",
    "train_dataset_cifar10, train_labels_cifar10 = reformat_data(c10_train_dataset, c10_train_labels, c10_image_size, c10_image_size, c10_image_depth)\n",
    "del c10_train_dataset # 删除原始特征\n",
    "del c10_train_labels # 删除原始label\n",
    "\n",
    "print(\"训练集包含以下标签: {}\".format(np.unique(c10_train_dict[b'labels'])))    # np.unique()该函数是去除数组中的重复数字，并进行排序之后输出\n",
    "print('训练集维度', train_dataset_cifar10.shape, train_labels_cifar10.shape)\n",
    "print('测试集维度', test_dataset_cifar10.shape, test_labels_cifar10.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-146b192c7890>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA/\", one_hot=True) #载入mnist数据,会自动下载数据\n",
    "#input_data.read_data_sets(\"/tmp/MNIST_data/\", False, one_hot=True) # 手动载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  55000\n",
      "Validating data size:  5000\n",
      "Testing data size:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size: \", mnist.train.num_examples) \n",
    "print (\"Validating data size: \", mnist.validation.num_examples) \n",
    "print (\"Testing data size: \", mnist.test.num_examples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images[0]) \n",
    "print(mnist.train.labels[0]) \n",
    "print(mnist.test.labels[0]) \n",
    "print(mnist.validation.labels[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3d8807fcc6b2>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_DATA/\", one_hot=True) # 载入数据，会自动下载\n",
    "\n",
    "\n",
    "INPUT_NODE = 784  ## 输入层节点数。对于MNIST数据集，这个就等于图片的像素。  \n",
    "OUTPUT_NODE = 10   # 输出层节点数。这个等于类别的数目。MNIST数据集中区分0-9十个数字，所以节点数是10\n",
    "LAYER1_NODE = 500   ## 隐藏层节点数。这里使用只有一个隐藏层的网络结构作为样例，这个隐藏层500个节点   \n",
    "                              \n",
    "BATCH_SIZE = 100     #一个训练batch中的训练数据个数。数字越小，训练过程越接近随机梯度下降。数字越大，越接近梯度下降。   \n",
    "\n",
    "# 模型相关的参数\n",
    "LEARNING_RATE_BASE = 0.8      # 基础的学习率 \n",
    "LEARNING_RATE_DECAY = 0.99    # 学习率的衰减率\n",
    "REGULARAZTION_RATE = 0.0001   # 描述模型复杂度的正则化项在损失函数中的系数。\n",
    "TRAINING_STEPS = 10000        # 训练轮数 \n",
    "MOVING_AVERAGE_DECAY = 0.99   # 滑动平均衰减率\n",
    "\n",
    "# 辅助函数。给定神经网络的输入和所有参数，计算神经网络的前向传播结果。\n",
    "# 定义了一个使用ReLU激活函数的三层全连接神经网络。通过加入隐藏层实现了多层网络结构，通过ReLU激活函数实现了去线性化。\n",
    "# 这个函数中也支持传入用于计算参数平均值的类，方便在测试时使用滑动平均模型。\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1) # relu(x*w+b)计算隐藏层的前向传播结果，使用了ReLU激活函数。\n",
    "        # 计算输出层的前向传播结果。因为在计算损失函数时会一起计算softmax函数，所以这里不需要加入激活函数。\n",
    "        # 而且不加入softmax不会影响预测结果。因为预测时使用的是不同类别对应节点输出值的相对大小，\n",
    "        # 有没有softmax层对最后分类结果的计算并没有影响。所以在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1, weights2) + biases2 # [relu(x*w+b)]*w2+b2\n",
    "\n",
    "    else:\n",
    "        # 使用滑动平均类\n",
    "        # 首先使用avg_class.average函数计算得出变量的滑动平均值。\n",
    "        # 然后计算相应的神经网络前向传播结果。\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)  \n",
    "    \n",
    "def train(mnist): # 训练\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input') # 占位符\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')  # 占位符\n",
    "    # 生成隐藏层的参数。\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1)) # 变量，权重矩阵1\n",
    "    # 截断的正态分布中输出随机值。 shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE])) # 变量，偏置1\n",
    "    # 生成输出层的参数。\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))  # 变量，权重矩阵2\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))  # 变量，偏置2\n",
    "\n",
    "    # 计算不含滑动平均类的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2) \n",
    "    \n",
    "    # 定义训练轮数及相关的滑动平均类 \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) # 就是采用滑动平均的方法更新参数\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables()) \n",
    "    # tf.trainable_variables 返回所有 当前计算图中 在获取变量时未标记 trainable=False 的变量集合\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2) # 含滑动平均的前向传播结果\n",
    "    \n",
    "    # 计算交叉熵及其平均值\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    #logits为神经网络输出层的输出，https://blog.csdn.net/ZJRN1027/article/details/80199248\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy) # 用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。\n",
    "    \n",
    "    # 损失函数的计算\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE) # 可以计算给定参数的L2正则化项\n",
    "    regularaztion = regularizer(weights1) + regularizer(weights2)   # 计算w1和w2的l2正则\n",
    "    loss = cross_entropy_mean + regularaztion  # 交叉熵均值+l2正则\n",
    "    \n",
    "    # 设置指数衰减的学习率。\n",
    "    learning_rate = tf.train.exponential_decay(  \n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True) # 指数衰减法\n",
    "    \n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) \n",
    "    # GradientDescentOptimizer优化类，minimize,操作节点，用于最小化loss，global_step非none就会对其进行自增\n",
    "    \n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]): \n",
    "        # 返回一个控制依赖的上下文管理器，使用with关键字可以让在这个上下文环境中的操作都在control_inputs 执行\n",
    "        train_op = tf.no_op(name='train') # tf.no_op()什么都不做，仅做为点位符使用控制边界。\n",
    "\n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1)) \n",
    "    # tf.argmax()将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组\n",
    "    # tf.equal()该换数是比较两个矩阵相同的下标的元素，若相等则返回Ture，若不相等则返回False。\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #求均值\n",
    "    \n",
    "    # 初始化回话并开始训练过程。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run() # 初始化变量\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels} # 特征，label，组成的验证集字典\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels} # 测试集字典\n",
    "        \n",
    "        # 循环的训练神经网络。\n",
    "        for i in range(TRAINING_STEPS):  # 循环训练\n",
    "            if i % 1000 == 0:  # 每1000步，采用验证集进行评估模型效果\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            \n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)  # 取batch\n",
    "            sess.run(train_op,feed_dict={x:xs,y_:ys}) # 训练\n",
    "\n",
    "        test_acc=sess.run(accuracy,feed_dict=test_feed)  # 训练完成后，在测试集上进行测试\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" %(TRAINING_STEPS, test_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), validation accuracy using average model is 0.0784 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9772 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.982 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9838 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 5000 training step(s), validation accuracy using average model is 0.9838 \n",
      "After 6000 training step(s), validation accuracy using average model is 0.9852 \n",
      "After 7000 training step(s), validation accuracy using average model is 0.9844 \n",
      "After 8000 training step(s), validation accuracy using average model is 0.9852 \n",
      "After 9000 training step(s), validation accuracy using average model is 0.986 \n",
      "After 10000 training step(s), test accuracy using average model is 0.9836\n"
     ]
    }
   ],
   "source": [
    "train(mnist)  训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), validation accuracy using average model is 0.1738 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9766 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.9834 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9826 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9832 \n",
      "After 5000 training step(s), validation accuracy using average model is 0.9838 \n",
      "After 6000 training step(s), validation accuracy using average model is 0.9844 \n",
      "After 7000 training step(s), validation accuracy using average model is 0.9852 \n",
      "After 8000 training step(s), validation accuracy using average model is 0.9858 \n",
      "After 9000 training step(s), validation accuracy using average model is 0.9856 \n",
      "After 10000 training step(s), test accuracy using average model is 0.9823\n"
     ]
    }
   ],
   "source": [
    "avg_class = None\n",
    "train(mnist) # 将滑动平均类设为none,进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-95404b5bc72d>:89: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Initialized with learning_rate 0.001\n",
      "step 0000 : loss is 143.88, accuracy on training set 6.25 %, accuracy on test set 9.08 %\n",
      "step 1000 : loss is 001.56, accuracy on training set 37.50 %, accuracy on test set 35.20 %\n",
      "step 2000 : loss is 001.92, accuracy on training set 31.25 %, accuracy on test set 38.52 %\n",
      "step 3000 : loss is 001.47, accuracy on training set 50.00 %, accuracy on test set 41.46 %\n",
      "step 4000 : loss is 001.85, accuracy on training set 43.75 %, accuracy on test set 44.01 %\n",
      "step 5000 : loss is 001.67, accuracy on training set 37.50 %, accuracy on test set 43.09 %\n",
      "step 6000 : loss is 001.12, accuracy on training set 68.75 %, accuracy on test set 45.46 %\n",
      "step 7000 : loss is 002.23, accuracy on training set 37.50 %, accuracy on test set 45.86 %\n",
      "step 8000 : loss is 001.73, accuracy on training set 37.50 %, accuracy on test set 46.12 %\n",
      "step 9000 : loss is 001.33, accuracy on training set 43.75 %, accuracy on test set 48.22 %\n",
      "step 10000 : loss is 001.43, accuracy on training set 62.50 %, accuracy on test set 45.35 %\n"
     ]
    }
   ],
   "source": [
    "LENET5_LIKE_BATCH_SIZE = 32  # lenet-5 batch大小\n",
    "LENET5_LIKE_FILTER_SIZE = 5  # lenet-5 filter大小\n",
    "LENET5_LIKE_FILTER_DEPTH = 16  # lenet-5 filter 深度\n",
    "LENET5_LIKE_NUM_HIDDEN = 120   # \n",
    "\n",
    "def variables_lenet5_like(filter_size = LENET5_LIKE_FILTER_SIZE, \n",
    "                          filter_depth = LENET5_LIKE_FILTER_DEPTH, \n",
    "                          num_hidden = LENET5_LIKE_NUM_HIDDEN,\n",
    "                          image_width = 32, image_height = 32, image_depth = 3, num_labels = 10):\n",
    "    \n",
    "    w1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, image_depth, filter_depth], stddev=0.1)) # 采用高斯分布创建w1矩阵\n",
    "    b1 = tf.Variable(tf.zeros([filter_depth])) # b1偏置\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([filter_size, filter_size, filter_depth, filter_depth], stddev=0.1)) # w2\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[filter_depth])) # b2\n",
    "   \n",
    "    w3 = tf.Variable(tf.truncated_normal([(image_width // 4)*(image_height // 4)*filter_depth , num_hidden], stddev=0.1)) # w3\n",
    "    b3 = tf.Variable(tf.constant(1.0, shape = [num_hidden])) # b3\n",
    "\n",
    "    w4 = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev=0.1)) # w4\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape = [num_hidden])) # b4\n",
    "    \n",
    "    w5 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) # w5\n",
    "    b5 = tf.Variable(tf.constant(1.0, shape = [num_labels])) # b5\n",
    "    variables = {\n",
    "        'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5,\n",
    "        'b1': b1, 'b2': b2, 'b3': b3, 'b4': b4, 'b5': b5\n",
    "    } # 组成字典\n",
    "    return variables # 返回变量集\n",
    "\n",
    "def model_lenet5_like(data, variables): # 定义lenet-5模型\n",
    "    layer1_conv = tf.nn.conv2d(data, variables['w1'], [1, 1, 1, 1], padding='SAME') #卷积核1，张量输入、卷积核参数、步长参数、\"SAME\"是考虑边界，不足的时候用0去填充周围\n",
    "\n",
    "    layer1_actv = tf.nn.relu(layer1_conv + variables['b1'])  # 卷积结果+偏置b1, 再使用relu激活函数\n",
    "    layer1_pool = tf.nn.avg_pool(layer1_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') \n",
    "    # 平均池化，需要做池化的输入图像、池化窗口的大小、不同维度上的步长（第三个参数strides：不同维度上的步长，是一个长度为4的一维向量，[ 1, strides, strides, 1]，第一维和最后一维的数字要求必须是1。因为卷积层的步长只对矩阵的长和宽有效。）\n",
    "\n",
    "    layer2_conv = tf.nn.conv2d(layer1_pool, variables['w2'], [1, 1, 1, 1], padding='SAME') # 卷积核2\n",
    "    layer2_actv = tf.nn.relu(layer2_conv + variables['b2']) # 卷积结果2+偏置b2，relu激活函数\n",
    "    layer2_pool = tf.nn.avg_pool(layer2_actv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') # 平均池化2\n",
    "    \n",
    "    flat_layer = flatten_tf_array(layer2_pool)  # 展成2维向量，k*l,k为样本数\n",
    "    layer3_fccd = tf.matmul(flat_layer, variables['w3']) + variables['b3'] # 全连接层，对展平的结果*w+b\n",
    "    layer3_actv = tf.nn.relu(layer3_fccd) # 使用relu激活\n",
    "    layer3_drop = tf.nn.dropout(layer3_actv, 0.5)  # tf.nn.dropout是TensorFlow里面为了防止或减轻过拟合而使用的函数，它一般用在全连接层.在不同的训练过程中随机扔掉一部分神经元\n",
    "    \n",
    "    layer4_fccd = tf.matmul(layer3_actv, variables['w4']) + variables['b4']  # 全连接层，*w4+b4\n",
    "    layer4_actv = tf.nn.relu(layer4_fccd) # relu激活\n",
    "    layer4_drop = tf.nn.dropout(layer4_actv, 0.5) # dropout\n",
    "    \n",
    "    logits = tf.matmul(layer4_actv, variables['w5']) + variables['b5'] # 全连接层，*w5+b5\n",
    "    return logits # 返回结果\n",
    "\n",
    "\n",
    "#Variables used in the constructing and running the graph\n",
    "num_steps = 10001 # 训练轮数\n",
    "display_step = 1000 # 展示步长\n",
    "learning_rate = 0.001 # 学习率\n",
    "batch_size = 16  # batch大小\n",
    "\n",
    "#定义数据的基本信息，传入变量\n",
    "image_width = 32  # 图片宽度\n",
    "image_height = 32  # 图片高度\n",
    "image_depth = 3    # 图片深度\n",
    "num_labels = 10    # label种数\n",
    "\n",
    "\n",
    "test_dataset = test_dataset_cifar10   # 测试特征\n",
    "test_labels = test_labels_cifar10     # 测试label\n",
    "train_dataset = train_dataset_cifar10  # 训练特征\n",
    "train_labels = train_labels_cifar10   # 训练label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()  # 创建图\n",
    "with graph.as_default():\n",
    "    #1 首先使用占位符定义数据变量的维度\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth)) # 占位符，用于训练特征\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))  # 占位符，用于训练label\n",
    "    tf_test_dataset = tf.constant(test_dataset, tf.float32)   # 测试集\n",
    "\n",
    "    #2 然后初始化权重矩阵和偏置向量\n",
    "    variables = variables_lenet5_like(image_width = image_width, image_height=image_height, image_depth = image_depth, num_labels = num_labels)\n",
    "\n",
    "\n",
    "    #3 使用模型计算分类\n",
    "    logits = model_lenet5_like(tf_train_dataset, variables)\n",
    "\n",
    "    #4 使用带softmax的交叉熵函数计算预测标签和真实标签之间的损失函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "\n",
    "    #5  采用Adam优化算法优化上一步定义的损失函数，给定学习率\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # 执行预测推断\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(model_lenet5_like(tf_test_dataset, variables))\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #初始化全部变量\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized with learning_rate', learning_rate)\n",
    "    for step in range(num_steps):  # 循环训练\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 计算偏移量\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :] # 训练集\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :] # 训练集label\n",
    "        #在每一次批量中，获取当前的训练数据，并传入feed_dict以馈送到占位符中\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} # 将训练特征和label组成字典，用于后面训练\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)#训练\n",
    "        train_accuracy = accuracy(predictions, batch_labels)#计算准确率\n",
    "        \n",
    "        if step % display_step == 0: # 每1000步在测试集上评估模型准确率，并打印\n",
    "            test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "            message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {:02.2f} %, accuracy on test set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy)\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
