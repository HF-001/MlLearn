{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设定神经网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_NODE = 784 # 输入层节点数，图像像素大小\n",
    "OUTPUT_NODE = 10 # 输出层节点数\n",
    "\n",
    "IMAGE_SIZE = 28  # 图片大小\n",
    "NUM_CHANNELS = 1 # 图片通道数，黑白的为1\n",
    "NUM_LABELS = 10   # label数\n",
    "\n",
    "CONV1_DEEP = 32  # 第一个卷积层深度，卷积核个数\n",
    "CONV1_SIZE = 5  # 第一个卷积层卷积核大小\n",
    "\n",
    "CONV2_DEEP = 64 #  第二个卷积层深度，卷积核个数\n",
    "CONV2_SIZE = 5  # 第二个卷积层卷积核大小\n",
    "\n",
    "FC_SIZE = 512  # 全连接层大小，节点个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义前向传播的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "正则表达式：输入层->(卷积层+->池化层？)+ ->全连接层+\n",
    "LeNet-5: 输入层->卷积层->池化层->卷积层->池化层->全连接层->全连接层->输出层\n",
    "\"\"\"\n",
    "def inference(input_tensor, train, regularizer):  # 前向传播函数\n",
    "    with tf.variable_scope('layer1-conv1'): # 2第一个卷积层，1是输入层\n",
    "        conv1_weights = tf.get_variable(  \n",
    "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))  # 权重\n",
    "        conv1_biases = tf.get_variable(\"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0)) # 偏置\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME') # 卷积\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases)) # relu激活函数\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):  # 3第一个池化层\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") # 池化\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\"): # 4第二个卷积层\n",
    "        conv2_weights = tf.get_variable(\n",
    "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1)) # 权重\n",
    "        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0)) # 偏置\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME') # 卷积\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))  # relu激活函数\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"): # 5 第二个池化层\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # 池化\n",
    "        pool_shape = pool2.get_shape().as_list() # 获取池化层的形状\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3] # 计算总的节点数\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes]) # 压平，展成一维向量\n",
    "\n",
    "    with tf.variable_scope('layer5-fc1'): # 6 第一个全连接层\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1)) # 权重\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights)) # 正则化项\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1)) # 偏置\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases) # relu激活函数，relu(x*w + b)\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5) # dropout正则化\n",
    "\n",
    "    with tf.variable_scope('layer6-fc2'):  # 7 第二个全连接层，也是输出层 \n",
    "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))  # 权重\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))   # 正则化项\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))  # 偏置\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases   # x*w + b\n",
    "\n",
    "    return logit  # 返回输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
